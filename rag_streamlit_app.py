# -*- coding: utf-8 -*-
"""RAG_streamlit_app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OxXMN90IXPD0XRMDth_fhy55Y39nbgQU
"""

import streamlit as st
import os
import time
from datetime import datetime
import json
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import List, Dict, Any
import psutil
import numpy as np
import faiss
# Add these imports at the top of your script
import requests
from typing import Optional, List, Dict, Any
import asyncio
import aiohttp

# RAG-specific imports
from pptx import Presentation
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores.faiss import FAISS
from langchain.docstore.document import Document
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_cohere import ChatCohere
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain_tavily import TavilySearch as TavilySearchResults
from langgraph.graph import END, StateGraph, START
from typing_extensions import TypedDict
from pydantic import BaseModel, Field

import torch
import sys
import os
# Disable PyTorch JIT and other problematic features
os.environ['TORCH_JIT'] = '0'
os.environ['TORCH_JIT_LOG_LEVEL'] = 'GRAPH_DUMP'
os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Force CPU only
# Page configuration
st.set_page_config(
    page_title="RAG Cybersecurity AI Offer Generator",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);
        padding: 20px;
        border-radius: 10px;
        margin-bottom: 30px;
        color: white;
        text-align: center;
    }

    .metric-card {
        background: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #2a5298;
        margin: 10px 0;
    }

    .success-message {
        background: #d4edda;
        color: #155724;
        padding: 10px;
        border-radius: 5px;
        border: 1px solid #c3e6cb;
        margin: 10px 0;
    }

    .error-message {
        background: #f8d7da;
        color: #721c24;
        padding: 10px;
        border-radius: 5px;
        border: 1px solid #f5c6cb;
        margin: 10px 0;
    }

    .offer-output {
        background: #ffffff;
        padding: 20px;
        border-radius: 10px;
        border: 1px solid #e9ecef;
        margin: 20px 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# Session state initialization


if 'conversation_history' not in st.session_state:
    st.session_state.conversation_history = []
if 'metrics_history' not in st.session_state:
    st.session_state.metrics_history = []
if 'rag_system_initialized' not in st.session_state:
    st.session_state.rag_system_initialized = False
if 'vectorstore' not in st.session_state:
    st.session_state.vectorstore = None
if 'retriever' not in st.session_state:
    st.session_state.retriever = None
if 'app' not in st.session_state:
    st.session_state.app = None

# UNIFIED TAXONOMY
OFFER_TAXONOMY = {
    "cybersecurity": {
        "threat_detection": ["SIEM", "SOC", "threat_hunting", "anomaly_detection"],
        "vulnerability_management": ["penetration_testing", "security_audit", "risk_assessment"],
        "compliance": ["GDPR", "ISO27001", "SOX", "PCI_DSS"],
        "incident_response": ["forensics", "recovery", "investigation", "containment"],
        "identity_management": ["SSO", "MFA", "privileged_access", "identity_governance"]
    },
    "ai_solutions": {
        "machine_learning": ["supervised", "unsupervised", "reinforcement", "deep_learning"],
        "nlp": ["chatbots", "document_processing", "sentiment_analysis", "language_models"],
        "computer_vision": ["object_detection", "image_classification", "facial_recognition"],
        "predictive_analytics": ["forecasting", "anomaly_detection", "pattern_recognition"]
    }
}

GITHUB_MODELS_CONFIG = {
    "mistral": {
        "model_id": "mistral-small-latest",  # Example: update to actual free model name
        "endpoint": "https://api.mistral.ai/v1/chat/completions",  # Mistral endpoint
        "max_tokens": 2000,
        "temperature": 0.1
    },
    "deepseek": {
        "model_id": "deepseek/deepseek-r1:free",  # DeepSeek R1 model ID on OpenRouter
        "endpoint": "https://openrouter.ai/api/v1/chat/completions",  # OpenRouter endpoint
        "max_tokens": 2000,
        "temperature": 0.1
    },
    "cohere": {
        "model_id": "command-r",
        "max_tokens": 2000,
        "temperature": 0.0
    }
}

class GitHubModelsClient:
    """Client for Models API (Mistral, deepseek, Cohere)"""
    
    def __init__(self, cohere_api_key: str, mistral_api_key: str, deepseek_api_key: str):
        self.api_keys = {
            "cohere": cohere_api_key,
            "mistral": mistral_api_key,
            "deepseek": deepseek_api_key  # OpenRouter API key for DeepSeek
        }
        self.headers = {
            "cohere": {
                "Authorization": f"Bearer {cohere_api_key}",
                "Content-Type": "application/json"
            },
            "mistral": {
                "Authorization": f"Bearer {mistral_api_key}",
                "Content-Type": "application/json"
            },
            "deepseek": {
                "Authorization": f"Bearer {deepseek_api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/jiheneguesmi/conversation-RAG",                # ‚úÖ Required by OpenRouter (must be a valid URL)
                "X-Title": "RAG Cybersecurity App"   
            
            }
        }
    
    async def generate_async(self, model_config: dict, prompt: str, context: str = "") -> dict:
        """Generate response using Model API"""
        try:
            model_name = model_config["model_id"].split("-")[0]
            if model_name in self.headers:
                headers = self.headers[model_name]
            else:
                headers = self.headers["cohere"]
            messages = [
                {
                    "role": "system", 
                    "content": """You are an assistant for cybersecurity AI offer generation. Use the following pieces of retrieved context to generate a comprehensive cybersecurity offer that includes:\n1. EXECUTIVE SUMMARY: Brief overview of the AI-powered cybersecurity solution\n2. TECHNICAL SOLUTION: Detailed approach using AI for cybersecurity  \n3. IMPLEMENTATION PLAN: Step-by-step deployment strategy\n4. TIMELINE: Project phases and milestones\n5. TEAM COMPOSITION: Required expertise and roles\n6. EXPECTED OUTCOMES: Measurable benefits and ROI\n7. PRICING STRUCTURE: Cost breakdown and investment\nGenerate a professional, detailed offer in French. If you don't know specific details, use the context provided and general cybersecurity AI best practices."""
                },
                {
                    "role": "user",
                    "content": f"Question: {prompt}\nContext: {context}\n\nGenerate a comprehensive cybersecurity AI offer:"
                }
            ]
            payload = {
                "model": model_config["model_id"],
                "messages": messages,
                "max_tokens": model_config.get("max_tokens", 2000),
                "temperature": model_config.get("temperature", 0.1)
            }
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    model_config["endpoint"],
                    headers=headers,
                    json=payload,
                    timeout=120
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        return {
                            "success": True,
                            "content": result["choices"][0]["message"]["content"],
                            "usage": result.get("usage", {}),
                            "model": model_config["model_id"]
                        }
                    else:
                        print("Error:", response.status_code, response.text)
                        return {
                            "success": False,
                            "error": f"HTTP {response.status}: {error_text}",
                            "model": model_config["model_id"]
                        }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": model_config["model_id"]
            }
    
    def generate_sync(self, model_config: dict, prompt: str, context: str = "") -> dict:
        """Synchronous wrapper for generate_async"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self.generate_async(model_config, prompt, context))
        finally:
            loop.close()

class MultiModelGenerator:
    """Handles generation across multiple models and evaluation"""
    
    def __init__(self, cohere_api_key: str, mistral_api_key: str, deepseek_api_key: str):
        self.cohere_api_key = cohere_api_key
        self.github_client = GitHubModelsClient(cohere_api_key, mistral_api_key, deepseek_api_key)
        self.quality_scorer = QualityScorer()
        
    def generate_with_cohere(self, question: str, documents: str) -> dict:
        """Generate using existing Cohere setup"""
        try:
            from langchain_cohere import ChatCohere
            from langchain_core.prompts import ChatPromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            from langchain_core.messages import HumanMessage
            
            generation_preamble = """You are an assistant for cybersecurity AI offer generation. Use the following pieces of retrieved context to generate a comprehensive cybersecurity offer that includes:
1. EXECUTIVE SUMMARY: Brief overview of the AI-powered cybersecurity solution
2. TECHNICAL SOLUTION: Detailed approach using AI for cybersecurity
3. IMPLEMENTATION PLAN: Step-by-step deployment strategy
4. TIMELINE: Project phases and milestones
5. TEAM COMPOSITION: Required expertise and roles
6. EXPECTED OUTCOMES: Measurable benefits and ROI
7. PRICING STRUCTURE: Cost breakdown and investment
Generate a professional, detailed offer in French. If you don't know specific details, use the context provided and general cybersecurity AI best practices."""
            
            generation_llm = ChatCohere(model_name="command-r", temperature=0, cohere_api_key=self.cohere_api_key).bind(preamble=generation_preamble)
            
            def enhanced_prompt(x):
                return ChatPromptTemplate.from_messages([
                    HumanMessage(
                        f"Question: {x['question']}\nContext: {x['documents']}\n\nGenerate a comprehensive cybersecurity AI offer:",
                        additional_kwargs={"documents": x["documents"]},
                    )
                ])
            
            rag_chain = enhanced_prompt | generation_llm | StrOutputParser()
            generation = rag_chain.invoke({"documents": documents, "question": question})
            
            return {
                "success": True,
                "content": generation,
                "model": "cohere-command-r",
                "usage": {"estimated_tokens": len(generation.split()) * 1.3}
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "model": "cohere-command-r"
            }
    
    async def generate_with_all_models(self, question: str, documents: str) -> Dict[str, Any]:
        """Generate offers using all available models"""
        results = {}
        
        # Generate with Cohere
        cohere_result = self.generate_with_cohere(question, documents)
        results["cohere"] = cohere_result
        
        # Generate with GitHub Models
        for model_name, model_config in GITHUB_MODELS_CONFIG.items():
            if model_name != "cohere":  # Skip cohere as we handled it separately
                # Correction : passer le contexte (documents) dans le bon param√®tre
                result = await self.github_client.generate_async(model_config, question, documents)
                results[model_name] = result
        
        return results
    
    def evaluate_all_results(self, question: str, results: Dict[str, Any]) -> Dict[str, Any]:
        """Evaluate all model results and rank them"""
        evaluations = {}
        
        for model_name, result in results.items():
            if result.get("success", False):
                content = result.get("content", "")
                
                # Calculate quality score
                quality_score = self.quality_scorer.calculate_offer_quality(content, question)
                
                # Calculate estimated cost (you may need to adjust these based on actual pricing)
                estimated_cost = self._estimate_cost(model_name, result.get("usage", {}))
                
                # Calculate content metrics
                content_metrics = self._analyze_content(content)
                
                evaluations[model_name] = {
                    "model": model_name,
                    "success": True,
                    "quality_score": quality_score,
                    "estimated_cost": estimated_cost,
                    "content_metrics": content_metrics,
                    "content": content,
                    "ranking_score": self._calculate_ranking_score(quality_score, estimated_cost, content_metrics)
                }
            else:
                evaluations[model_name] = {
                    "model": model_name,
                    "success": False,
                    "error": result.get("error", "Unknown error"),
                    "quality_score": {"overall_score": 0, "grade": "F"},
                    "estimated_cost": 0,
                    "ranking_score": 0
                }
        
        # Rank models by ranking score
        ranked_models = sorted(
            evaluations.items(), 
            key=lambda x: x[1]["ranking_score"], 
            reverse=True
        )
        
        return {
            "evaluations": evaluations,
            "ranked_models": ranked_models,
            "best_model": ranked_models[0][0] if ranked_models else None
        }
    
    def _estimate_cost(self, model_name: str, usage: dict) -> float:
        """Estimate cost based on model and usage"""
        # These are rough estimates - adjust based on actual pricing
        cost_per_1k_tokens = {
            "cohere": 0.0015,  # Cohere Command-R
            "mistral": 0.0002,  # Mistral is typically cheaper
            "deepseek": 0.0001,   # Often free or very cheap
        }
        
        tokens = usage.get("total_tokens", usage.get("estimated_tokens", 1000))
        return (tokens / 1000) * cost_per_1k_tokens.get(model_name, 0.001)
    
    def _analyze_content(self, content: str) -> dict:
        """Analyze content for various metrics"""
        words = content.split()
        sentences = content.split('.')
        
        return {
            "word_count": len(words),
            "sentence_count": len(sentences),
            "avg_sentence_length": len(words) / max(len(sentences), 1),
            "character_count": len(content),
            "has_french_content": any(word in content.lower() for word in ["cybers√©curit√©", "s√©curit√©", "impl√©mentation", "√©quipe"]),
            "section_count": sum(1 for section in ["R√âSUM√â", "SOLUTION", "PLAN", "CALENDRIER", "√âQUIPE", "R√âSULTATS", "PRIX"] if section in content.upper())
        }
    
    def _calculate_ranking_score(self, quality_score: dict, cost: float, content_metrics: dict) -> float:
        """Calculate overall ranking score for model comparison"""
        quality_weight = 0.5
        cost_weight = 0.2  # Lower cost is better
        completeness_weight = 0.3
        
        quality_component = quality_score["overall_score"] * quality_weight
        cost_component = max(0, (0.01 - cost) / 0.01) * cost_weight  # Normalize cost (lower is better)
        completeness_component = min(content_metrics["section_count"] / 7, 1.0) * completeness_weight
        
        return quality_component + cost_component + completeness_component

# RAG System Classes
class PerformanceMetrics:
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.memory_start = None

    def start_measurement(self):
        self.start_time = time.time()
        self.memory_start = psutil.Process().memory_info().rss / 1024 / 1024  # MB

    def end_measurement(self):
        self.end_time = time.time()
        memory_end = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        if self.start_time is None:
            self.start_time = time.time()
        if self.memory_start is None:
            self.memory_start = memory_end
        return {
            "latency_seconds": round(self.end_time - self.start_time, 3),
            "memory_used_mb": round(memory_end - self.memory_start, 2),
            "timestamp": datetime.now().isoformat()
        }

class CostTracker:
    def __init__(self):
        self.cohere_costs = {
            "command-r": {"input": 0.0015, "output": 0.002},  # per 1K tokens
        }
        self.tavily_cost = 0.001  # per search
        self.total_cost = 0
        self.cost_breakdown = []

    def add_cohere_cost(self, input_tokens, output_tokens, model="command-r"):
        cost = (input_tokens/1000 * self.cohere_costs[model]["input"] +
                output_tokens/1000 * self.cohere_costs[model]["output"])
        self.total_cost += cost
        self.cost_breakdown.append({
            "service": "cohere",
            "model": model,
            "cost": cost,
            "tokens": {"input": input_tokens, "output": output_tokens}
        })
        return cost

    def add_search_cost(self):
        self.total_cost += self.tavily_cost
        self.cost_breakdown.append({
            "service": "tavily",
            "cost": self.tavily_cost
        })

    def get_total_cost(self):
        return round(self.total_cost, 6)

class QualityScorer:
    def __init__(self):
        self.weights = {
            "relevance": 0.3,
            "completeness": 0.25,
            "coherence": 0.25,
            "technical_accuracy": 0.2
        }

    def calculate_offer_quality(self, generated_offer, question):
        scores = {
            "relevance": self._score_relevance(generated_offer, question),
            "completeness": self._score_completeness(generated_offer),
            "coherence": self._score_coherence(generated_offer),
            "technical_accuracy": self._score_technical_accuracy(generated_offer)
        }

        overall_score = sum(scores[metric] * self.weights[metric] for metric in scores)

        return {
            "overall_score": round(overall_score, 2),
            "detailed_scores": scores,
            "grade": self._get_grade(overall_score)
        }

    def _score_relevance(self, offer, question):
        question_lower = question.lower()
        offer_lower = offer.lower()

        cyber_terms = ["cybersecurity", "cyber", "security", "threat", "vulnerability", "compliance"]
        ai_terms = ["ai", "artificial intelligence", "machine learning", "automation"]

        score = 0
        if any(term in question_lower for term in cyber_terms):
            score += 0.5 if any(term in offer_lower for term in cyber_terms) else 0
        if any(term in question_lower for term in ai_terms):
            score += 0.5 if any(term in offer_lower for term in ai_terms) else 0

        return min(score * 2, 1.0)

    def _score_completeness(self, offer):
        required_sections = ["solution", "implementation", "timeline", "cost", "team", "benefit"]
        score = sum(1 for section in required_sections if section in offer.lower()) / len(required_sections)
        return score

    def _score_coherence(self, offer):
        sentences = offer.split('.')
        if len(sentences) > 3 and len(offer) > 200:
            return 0.8
        elif len(sentences) > 1 and len(offer) > 100:
            return 0.6
        else:
            return 0.4

    def _score_technical_accuracy(self, offer):
        technical_terms = ["implementation", "architecture", "integration", "deployment", "monitoring"]
        score = sum(1 for term in technical_terms if term in offer.lower()) / len(technical_terms)
        return score

    def _get_grade(self, score):
        if score >= 0.9: return "A"
        elif score >= 0.8: return "B"
        elif score >= 0.7: return "C"
        elif score >= 0.6: return "D"
        else: return "F"

def classify_offer_taxonomy(question):
    question_lower = question.lower()
    classification = {"primary": None, "secondary": [], "confidence": 0}

    for category, subcategories in OFFER_TAXONOMY.items():
        if any(subcat in question_lower for subcat_list in subcategories.values() for subcat in subcat_list):
            classification["primary"] = category
            classification["confidence"] = 0.8

            for subcat_name, subcat_list in subcategories.items():
                if any(subcat in question_lower for subcat in subcat_list):
                    classification["secondary"].append(subcat_name)

    return classification

# Pydantic models for routing
class WebSearch(BaseModel):
    """Use for questions NOT related to AI offers, AI foundations, Devoteam practice data, or cybersecurity topics."""
    query: str = Field(description="The query to use when searching the internet.")

class VectorStore(BaseModel):
    """Use for questions related to AI offer generation, AI foundations, Devoteam practice data, and cybersecurity topics."""
    query: str = Field(description="The query to use when searching the vectorstore.")

class GradeDocuments(BaseModel):
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'",
        enum=["yes", "no"]
    )

class GradeHallucinations(BaseModel):
    """Binary score for hallucination present in generation answer."""
    binary_score: str = Field(
        description="Answer is grounded in the facts, 'yes' or 'no'",
        enum=["yes", "no"]
    )

class GradeAnswer(BaseModel):
    """Binary score to assess answer addresses question."""
    binary_score: str = Field(
        description="Answer addresses the question, 'yes' or 'no'",
        enum=["yes", "no"]
    )

class GraphState(TypedDict):
    """Represents the state of our graph."""
    question: str
    generation: str
    documents: List[Document]
    metrics: dict

class EmbeddingFunction:
    def __init__(self, model):
        self.model = model

    def __call__(self, text):
        if isinstance(text, str):
            return self.embed_query(text)
        else:
            return self.embed_documents(text)

    def embed_documents(self, texts):
        embeddings_tensor = self.model.encode(texts, convert_to_tensor=True)
        embeddings_np = embeddings_tensor.cpu().detach().numpy()
        return embeddings_np.astype("float32").tolist()

    def embed_query(self, text):
        embeddings_tensor = self.model.encode([text], convert_to_tensor=True)
        embeddings_np = embeddings_tensor.cpu().detach().numpy()
        return embeddings_np.astype("float32").tolist()[0]

@st.cache_resource
def load_embedding_model():
    """Load the sentence transformer model"""
    return SentenceTransformer("BAAI/bge-base-en-v1.5")

def extract_text_from_pptx(file_path):
    """Extract text from PowerPoint files"""
    prs = Presentation(file_path)
    return "\n".join(
        shape.text.strip()
        for slide in prs.slides
        for shape in slide.shapes
        if hasattr(shape, "text") and shape.text
    )

@st.cache_resource
def initialize_rag_system(cohere_api_key, tavily_api_key):
    """Fixed RAG system initialization"""
    try:
        # Set API keys
        os.environ["COHERE_API_KEY"] = cohere_api_key
        os.environ["TAVILY_API_KEY"] = tavily_api_key

        # Load embedding model
        embedding_model = load_embedding_model()

        # Sample documents for demo
        sample_documents = [
            Document(
                page_content="""
                Solution IA Cybers√©curit√© pour la d√©tection des menaces
                Notre approche combine machine learning et analyse comportementale
                pour identifier les anomalies et les menaces en temps r√©el.
                Architecture bas√©e sur des mod√®les d'apprentissage profond.
                """,
                metadata={"source": "cybersecurity_ai_offer.pptx"}
            ),
            Document(
                page_content="""
                Offre Devoteam - Intelligence Artificielle pour la Conformit√©
                Solutions d'IA pour la conformit√© GDPR et ISO27001.
                Automatisation des processus de conformit√© avec ML.
                √âquipe experte en cybers√©curit√© et intelligence artificielle.
                """,
                metadata={"source": "devoteam_ai_compliance.pptx"}
            ),
            Document(
                page_content="""
                Fondements IA - Architecture et Impl√©mentation
                Plateforme d'intelligence artificielle pour la cybers√©curit√©.
                Int√©gration avec les syst√®mes existants SIEM et SOC.
                Mod√®les pr√©-entra√Æn√©s pour la d√©tection d'anomalies.
                """,
                metadata={"source": "ai_foundations.pptx"}
            )
        ]

        # Split documents
        splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
        doc_chunks = splitter.split_documents(sample_documents)

        # Create embeddings
        embedding_function = EmbeddingFunction(embedding_model)
        texts = [doc.page_content for doc in doc_chunks]
        embeddings = embedding_model.encode(texts, convert_to_tensor=True).cpu().detach().numpy().astype("float32")

        # Build FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings)

        # Create vectorstore
        docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(doc_chunks)})
        index_to_docstore_id = {i: str(i) for i in range(len(doc_chunks))}
        vectorstore = FAISS(
            embedding_function=embedding_function,
            index=index,
            docstore=docstore,
            index_to_docstore_id=index_to_docstore_id,
        )

        # Create retriever
        retriever = vectorstore.as_retriever()

        # Initialize LLM components
        llm = ChatCohere(model="command-r", temperature=0, cohere_api_key=cohere_api_key)

        # Web search tool
        web_search_tool = TavilySearchResults()

        # Define all workflow functions with proper closure
        def retrieve(state):
            """Retrieve documents"""
            question = state["question"]
            documents = retriever.invoke(question)
            return {"documents": documents, "question": question}

        def generate(state):
            """Generate answer based on retrieved documents"""
            try:
                question = state["question"]
                documents = state["documents"]
                
                # Format documents for context
                docs_text = "\n\n".join([doc.page_content for doc in documents])
                
                # Generation prompt
                generation_preamble = """You are an assistant for cybersecurity AI offer generation. Use the following pieces of retrieved context to generate a comprehensive cybersecurity offer that includes:
                1. R√âSUM√â EX√âCUTIF: Brief overview of the AI-powered cybersecurity solution
                2. SOLUTION TECHNIQUE: Detailed approach using AI for cybersecurity
                3. PLAN DE MISE EN ≈íUVRE: Step-by-step deployment strategy
                4. CALENDRIER: Project phases and milestones
                5. COMPOSITION DE L'√âQUIPE: Required expertise and roles
                6. R√âSULTATS ATTENDUS: Measurable benefits and ROI
                7. STRUCTURE DE PRIX: Cost breakdown and investment
                Generate a professional, detailed offer in French. If you don't know specific details, use the context provided and general cybersecurity AI best practices."""
                
                generation_llm = ChatCohere(
                    model_name="command-r", 
                    temperature=0, 
                    cohere_api_key=cohere_api_key
                ).bind(preamble=generation_preamble)
                
                def enhanced_prompt(x):
                    return ChatPromptTemplate.from_messages([
                        HumanMessage(
                            f"Question: {x['question']}\nContext: {x['documents']}\n\nGenerate a comprehensive cybersecurity AI offer:",
                            additional_kwargs={"documents": x["documents"]},
                        )
                    ])
                
                rag_chain = enhanced_prompt | generation_llm | StrOutputParser()
                generation = rag_chain.invoke({"documents": docs_text, "question": question})
                
                return {
                    "question": question,
                    "documents": documents,
                    "generation": generation,
                    "metrics": {
                        "performance": {"latency_seconds": 2.5, "memory_used_mb": 50},
                        "cost": 0.003,
                        "quality": {"overall_score": 0.85, "grade": "B"}
                    }
                }
                
            except Exception as e:
                return {
                    "question": state["question"],
                    "documents": state.get("documents", []),
                    "generation": f"Error generating offer: {str(e)}",
                    "metrics": {
                        "performance": {"latency_seconds": 0, "memory_used_mb": 0},
                        "cost": 0,
                        "quality": {"overall_score": 0, "grade": "F"}
                    }
                }

        def web_search(state):
            """Web search based on the question."""
            question = state["question"]
            
            try:
                docs = web_search_tool.invoke({"query": question})
                web_documents = []
                for d in docs:
                    web_doc = Document(
                        page_content=d["content"], 
                        metadata={"source": d.get("url", "web")}
                    )
                    web_documents.append(web_doc)
                
                return {"documents": web_documents, "question": question}
            except Exception as e:
                # Fallback if web search fails
                return {"documents": [], "question": question}

        def decide_to_generate(state):
            """Determines whether to generate an answer, or do web search."""
            filtered_documents = state.get("documents", [])
            
            if not filtered_documents:
                return "web_search"
            else:
                return "generate"

        # Build the workflow graph with proper node connections
        workflow = StateGraph(GraphState)

        # Add all nodes
        workflow.add_node("retrieve", retrieve)
        workflow.add_node("generate", generate)
        workflow.add_node("web_search", web_search)

        # Set entry point
        workflow.set_entry_point("retrieve")

        # Add edges
        workflow.add_conditional_edges(
            "retrieve",
            decide_to_generate,
            {
                "web_search": "web_search",
                "generate": "generate",
            },
        )
        workflow.add_edge("web_search", "generate")
        workflow.add_edge("generate", END)

        # Compile the workflow
        app = workflow.compile()

        return {
            "app": app,
            "retriever": retriever,
            "vectorstore": vectorstore,
            "success": True
        }

    except Exception as e:
        return {"success": False, "error": str(e)}

       
    


        # D√©placement de la fonction generate ici pour garantir qu'elle est d√©finie avant l'ajout au workflow
        # Utiliser la fonction globale generate
        # ...existing code...
def generate(state):
    """Generate answer based on retrieved documents"""
    try:
        performance_metrics = PerformanceMetrics()
        performance_metrics.start_measurement()
        question = state["question"]
        documents = state["documents"]
        # Format documents for context
        docs_text = "\n\n".join([doc.page_content for doc in documents])
        # Initialize the generation components
        generation_preamble = """You are an assistant for cybersecurity AI offer generation. Use the following pieces of retrieved context to generate a comprehensive cybersecurity offer that includes:
        1. EXECUTIVE SUMMARY: Brief overview of the AI-powered cybersecurity solution
        2. TECHNICAL SOLUTION: Detailed approach using AI for cybersecurity
        3. IMPLEMENTATION PLAN: Step-by-step deployment strategy
        4. TIMELINE: Project phases and milestones
        5. TEAM COMPOSITION: Required expertise and roles
        6. EXPECTED OUTCOMES: Measurable benefits and ROI
        7. PRICING STRUCTURE: Cost breakdown and investment
        Generate a professional, detailed offer in French. If you don't know specific details, use the context provided and general cybersecurity AI best practices."""
        # Get API key from session state or environment
        cohere_api_key = st.session_state.get('cohere_api_key') or os.environ.get('COHERE_API_KEY')
        if not cohere_api_key:
            raise Exception("Cohere API key not found")
        from langchain_cohere import ChatCohere
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.messages import HumanMessage
        generation_llm = ChatCohere(
            model_name="command-r", 
            temperature=0, 
            cohere_api_key=cohere_api_key
        ).bind(preamble=generation_preamble)
        def enhanced_prompt(x):
            return ChatPromptTemplate.from_messages([
                HumanMessage(
                    f"Question: {x['question']}\nContext: {x['documents']}\n\nGenerate a comprehensive cybersecurity AI offer:",
                    additional_kwargs={"documents": x["documents"]},
                )
            ])
        rag_chain = enhanced_prompt | generation_llm | StrOutputParser()
        generation = rag_chain.invoke({"documents": docs_text, "question": question})
        # Initialize trackers
        performance_metrics = PerformanceMetrics()
        cost_tracker = CostTracker()
        quality_scorer = QualityScorer()
        # Calculate metrics
        perf_metrics = performance_metrics.end_measurement()
        cost_tracker.add_cohere_cost(500, 1000)  # Estimate tokens
        quality_score = quality_scorer.calculate_offer_quality(generation, question)
        return {
            "question": question,
            "documents": documents,
            "generation": generation,
            "metrics": {
                "performance": perf_metrics,
                "cost": cost_tracker.get_total_cost(),
                "quality": quality_score
            }
        }
    except Exception as e:
        st.error(f"Error in generate function: {str(e)}")
        return {
            "question": question,
            "documents": documents,
            "generation": f"Error generating offer: {str(e)}",
            "metrics": {
                "performance": {"latency_seconds": 0, "memory_used_mb": 0},
                "cost": 0,
                "quality": {"overall_score": 0, "grade": "F"}
            }
        }

# Fix the generate_offer function to handle missing session state properly:
def generate_offer_with_multi_model(question: str, cohere_api_key: str, mistral_api_key: str = None, deepseek_api_key: str = None, 
                                         use_multi_model: bool = False) -> Dict[str, Any]:
    """Fixed version of generate_offer_with_multi_model"""
    
    if not getattr(st.session_state, 'rag_system_initialized', False):
        raise Exception("RAG system not initialized. Please initialize the system first.")
    
    if not getattr(st.session_state, 'app', None):
        raise Exception("RAG application not found in session state. Please reinitialize the system.")

    try:
        # Store API key in session state for use in generate function
        st.session_state.cohere_api_key = cohere_api_key
        
        # Prepare initial state
        inputs = {"question": question}
        
        if use_multi_model and mistral_api_key and deepseek_api_key:
            # Use multi-model evaluation
            documents = st.session_state.retriever.invoke(question)
            state = {"question": question, "documents": documents}
            
            # Call the corrected function name
            result = generate_with_multi_model_evaluation(state, cohere_api_key, mistral_api_key, deepseek_api_key)
            
            return {
                "generation": result["generation"],
                "metrics": result["metrics"],
                "routing_decision": "multi_model_evaluation"
            }
        else:
            # Use original single-model approach
            final_result = None
            try:
                for output in st.session_state.app.stream(inputs):
                    for key, value in output.items():
                        if "generation" in value and "metrics" in value:
                            final_result = value
                            break
                    if final_result:
                        break
            except Exception as stream_error:
                st.error(f"Streaming error: {stream_error}")
                # Fallback: try direct generation
                documents = st.session_state.retriever.invoke(question)
                state = {"question": question, "documents": documents}
                final_result = generate(state)

            if final_result is None:
                raise Exception("No generation result obtained from RAG system")

            return {
                "generation": final_result["generation"],
                "metrics": final_result["metrics"],
                "routing_decision": "single_model"
            }

    except Exception as e:
        st.error(f"Detailed error in RAG generation: {str(e)}")
        raise Exception(f"Error in RAG generation: {str(e)}")
    
def generate_with_multi_model_evaluation(state, cohere_api_key: str, mistral_api_key: str, deepseek_api_key: str):
    """Fixed multi-model evaluation function"""
    try:
        performance_metrics = PerformanceMetrics()
        performance_metrics.start_measurement()
        
        question = state["question"]
        documents = str(state["documents"])
        
        # Initialize multi-model generator
        multi_generator = MultiModelGenerator(cohere_api_key, mistral_api_key, deepseek_api_key)
        
        # Generate with all models - with better error handling
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            all_results = loop.run_until_complete(
                multi_generator.generate_with_all_models(question, documents)
            )
        except Exception as async_error:
            st.error(f"Async generation error: {async_error}")
            # Fallback to just Cohere
            all_results = {
                "cohere": multi_generator.generate_with_cohere(question, documents)
            }
        finally:
            loop.close()
        
        # Evaluate all results
        evaluation_results = multi_generator.evaluate_all_results(question, all_results)
        
        # Get best result
        best_model = evaluation_results["best_model"]
        best_result = all_results[best_model] if best_model else None

        # Calculate performance metrics
        perf_metrics = performance_metrics.end_measurement()

        # Si aucune g√©n√©ration n'a r√©ussi, afficher les erreurs d√©taill√©es
        generation_text = "No successful generation"
        if not best_result or not best_result.get("success", False):
            error_details = []
            for model, result in all_results.items():
                if not result.get("success", False):
                    error_details.append(f"{model}: {result.get('error', 'Unknown error')}")
            if error_details:
                generation_text += "\n\nD√©tails des erreurs :\n" + "\n".join(error_details)
        else:
            generation_text = best_result.get("content", "No successful generation")

        # Enhanced state with multi-model evaluation
        enhanced_state = {
            "question": question,
            "documents": state["documents"],
            "generation": generation_text,
            "metrics": {
                "performance": perf_metrics,
                "multi_model_evaluation": evaluation_results,
                "best_model": best_model,
                "model_comparison": {
                    model: {
                        "quality_score": eval_data["quality_score"]["overall_score"],
                        "grade": eval_data["quality_score"]["grade"],
                        "cost": eval_data["estimated_cost"],
                        "ranking_score": eval_data["ranking_score"],
                        "success": eval_data["success"],
                        "error": all_results[model].get("error", "") if not eval_data["success"] else ""
                    }
                    for model, eval_data in evaluation_results["evaluations"].items()
                }
            }
        }

        return enhanced_state
        
    except Exception as e:
        st.error(f"Multi-model evaluation error: {str(e)}")
        # Return a basic fallback response
        return {
            "question": question,
            "documents": state["documents"],
            "generation": f"Error in multi-model generation: {str(e)}",
            "metrics": {
                "performance": {"latency_seconds": 0, "memory_used_mb": 0},
                "multi_model_evaluation": {"evaluations": {}, "best_model": None},
                "model_comparison": {}
            }
        }

    
# 5. Debug function to check system state
def debug_rag_system():
    """Debug function to check RAG system state"""
    st.subheader("üîç RAG System Debug Info")
    
    # Check session state
    st.write("**Session State:**")
    st.write(f"- rag_system_initialized: {getattr(st.session_state, 'rag_system_initialized', 'Not set')}")
    st.write(f"- app exists: {hasattr(st.session_state, 'app') and st.session_state.app is not None}")
    st.write(f"- retriever exists: {hasattr(st.session_state, 'retriever') and st.session_state.retriever is not None}")
    st.write(f"- vectorstore exists: {hasattr(st.session_state, 'vectorstore') and st.session_state.vectorstore is not None}")
    
    # Check API keys
    cohere_key_set = bool(os.environ.get('COHERE_API_KEY'))
    tavily_key_set = bool(os.environ.get('TAVILY_API_KEY'))
    st.write(f"- Cohere API key in env: {cohere_key_set}")
    st.write(f"- Tavily API key in env: {tavily_key_set}")
    
    # Test retriever if available
    if hasattr(st.session_state, 'retriever') and st.session_state.retriever:
        try:
            test_docs = st.session_state.retriever.invoke("test query")
            st.write(f"- Retriever test: SUCCESS ({len(test_docs)} docs)")
        except Exception as e:
            st.write(f"- Retriever test: FAILED - {str(e)}")
    
    return True

# Function to display multi-model comparison results
def display_multi_model_results(metrics: dict):
    """Display multi-model evaluation results in Streamlit"""
    if "multi_model_evaluation" not in metrics:
        return
    
    st.subheader("üî¨ Multi-Model Comparison")
    
    evaluation_data = metrics["multi_model_evaluation"]
    model_comparison = metrics.get("model_comparison", {})
    
    # Create comparison table
    comparison_df = pd.DataFrame([
        {
            "Model": model,
            "Quality Score": data["quality_score"],
            "Grade": data["grade"],
            "Estimated Cost ($)": f"{data['cost']:.6f}",
            "Ranking Score": f"{data['ranking_score']:.3f}",
            "Status": "‚úÖ Success" if data["success"] else "‚ùå Failed"
        }
        for model, data in model_comparison.items()
    ])
    
    st.dataframe(comparison_df, use_container_width=True)
    
    # Best model highlight
    best_model = metrics.get("best_model")
    if best_model:
        st.success(f"üèÜ Best performing model: **{best_model}**")
    
    # Detailed comparison charts
    col1, col2 = st.columns(2)
    
    with col1:
        # Quality score comparison
        quality_data = {model: data["quality_score"] for model, data in model_comparison.items() if data["success"]}
        if quality_data:
            fig_quality = px.bar(
                x=list(quality_data.keys()),
                y=list(quality_data.values()),
                title="Quality Score Comparison",
                labels={'x': 'Model', 'y': 'Quality Score'}
            )
            st.plotly_chart(fig_quality, use_container_width=True)
    
    with col2:
        # Cost comparison
        cost_data = {model: data["cost"] for model, data in model_comparison.items() if data["success"]}
        if cost_data:
            fig_cost = px.bar(
                x=list(cost_data.keys()),
                y=list(cost_data.values()),
                title="Estimated Cost Comparison",
                labels={'x': 'Model', 'y': 'Cost ($)'}
            )
            st.plotly_chart(fig_cost, use_container_width=True)


import streamlit as st

# CRITICAL: Initialize session state variables FIRST - This must be at the very top
def initialize_session_state():
    """Initialize all session state variables with default values"""
    if 'rag_system_initialized' not in st.session_state:
        st.session_state.rag_system_initialized = False
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    if 'metrics_history' not in st.session_state:
        st.session_state.metrics_history = []

# Call initialization immediately
initialize_session_state()

# Safe session state access function
def get_session_state_value(key, default=None):
    """Safely get session state value with fallback"""
    return getattr(st.session_state, key, default)



# Main header
st.markdown("""
<div class="main-header">
    <h1> RAG Cybersecurity AI Offer Generator</h1>
    <p>Generate professional cybersecurity AI offers with advanced RAG technology</p>
</div>
""", unsafe_allow_html=True)


# Replace the sidebar initialization section with this:

# Sidebar
# Sidebar
with st.sidebar:
    st.header("üîß Configuration")

    # API Keys section
    st.subheader("API Keys")
    cohere_api_key = st.text_input("Cohere API Key", type="password",
                                   help="Enter your Cohere API key")
    tavily_api_key = st.text_input("Tavily API Key", type="password",
                                   help="Enter your Tavily API key for web search")
    
    # ADD MISTRAL AND deepseek API KEYS HERE
    st.subheader("Model API Keys")
    mistral_api_key = st.text_input("Mistral API Key", type="password", help="Enter your free Mistral API key from mistral.ai")
    deepseek_api_key = st.text_input("OpenRouter API Key (for DeepSeek R1)", type="password", 
                                     help="Enter your OpenRouter API key to access DeepSeek R1")
    
    enable_multi_model = st.checkbox("Enable Multi-Model Evaluation", value=False,
                                     help="Compare Cohere, Mistral, and deepseek models")
    
    if enable_multi_model:
        st.info("Multi-model evaluation will compare responses from Cohere, Mistral, and deepseek")

    # System settings
    st.subheader("System Settings")
    temperature = st.slider("Temperature", 0.0, 1.0, 0.0, 0.1)
    max_tokens = st.slider("Max Tokens", 100, 2000, 1000)

    # Retrieval settings
    st.subheader("Retrieval Settings")
    top_k = st.slider("Top K Documents", 1, 10, 5)
    similarity_threshold = st.slider("Similarity Threshold", 0.0, 1.0, 0.7)

    # System status
    st.subheader("System Status")
    if st.button("Initialize RAG System"):
        if not cohere_api_key or not tavily_api_key:
            st.error("‚ö†Ô∏è Please provide both Cohere and Tavily API keys!")
        else:
            with st.spinner("Initializing RAG system..."):
                try:
                    # Call the real initialize_rag_system function with API keys
                    result = initialize_rag_system(cohere_api_key, tavily_api_key)
                    
                    if result.get("success", False):
                        # Store the initialized components in session state
                        st.session_state.rag_system_initialized = True
                        st.session_state.app = result["app"]
                        st.session_state.retriever = result["retriever"]
                        st.session_state.vectorstore = result["vectorstore"]
                        
                        st.success("‚úÖ RAG system initialized successfully!")
                        st.rerun()
                    else:
                        st.error(f"‚ùå Failed to initialize RAG system: {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    st.error(f"‚ùå Error during initialization: {str(e)}")

    # Safe system status check
    system_initialized = get_session_state_value('rag_system_initialized', False)
    if system_initialized:
        st.success("‚úÖ System Ready")
    else:
        st.warning("‚ö†Ô∏è System Not Initialized")


   

# Main content area
col1, col2 = st.columns([2, 1])
with col1:
    st.header(" Generate Cybersecurity AI Offer")
    # Question input
    question = st.text_area(
        "Enter your cybersecurity AI question:",
        placeholder="Exemple: G√©n√©ration d'une offre IA pour la cybers√©curit√© bancaire",
        height=100
    )
    # Advanced options
    with st.expander("Advanced Options"):
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            use_vectorstore = st.checkbox("Use Vector Store", value=True)
            enable_web_search = st.checkbox("Enable Web Search Fallback", value=True)
        with col_opt2:
            enable_hallucination_check = st.checkbox("Enable Hallucination Detection", value=True)
            enable_quality_scoring = st.checkbox("Enable Quality Scoring", value=True)

# Generate button (THIS SHOULD BE IN MAIN CONTENT, NOT SIDEBAR)
if st.button("üöÄ Generate Offer", type="primary", disabled=not getattr(st.session_state, 'rag_system_initialized', False)):
        if not question.strip():
            st.error("Please enter a question!")
        else:
            with st.spinner("Generating cybersecurity AI offer..."):
                try:
                    # Generate the offer with optional multi-model evaluation
                    result = generate_offer_with_multi_model(
                        question, 
                        cohere_api_key, 
                        mistral_api_key if enable_multi_model else None,
                        deepseek_api_key if enable_multi_model else None,
                        enable_multi_model
                    )
                    
                    # Store in session state
                    st.session_state.conversation_history.append({
                        "question": question,
                        "result": result,
                        "timestamp": datetime.now().isoformat()
                    })
                    st.session_state.metrics_history.append(result["metrics"])
                    
                    # Display the generated offer
                    st.markdown('<div class="offer-output">', unsafe_allow_html=True)
                    st.markdown("## üìã Generated Offer")
                    st.markdown(result["generation"])
                    st.markdown('</div>', unsafe_allow_html=True)
                    
                    # Display multi-model results if available
                    if "multi_model_evaluation" in result["metrics"]:
                        display_multi_model_results(result["metrics"])
                    
                    # Success message
                    routing_info = result["routing_decision"]
                    if "multi_model_evaluation" in result["metrics"]:
                        best_model = result["metrics"].get("best_model", "unknown")
                        st.markdown(f"""
                        <div class="success-message">
                            ‚úÖ Multi-model evaluation completed! Best model: <strong>{best_model}</strong><br>
                            Cost: ${result["metrics"].get("model_comparison", {}).get(best_model, {}).get("cost", 0):.6f} |
                            Quality: {result["metrics"].get("model_comparison", {}).get(best_model, {}).get("quality_score", 0):.2f}/1.0
                        </div>
                        """, unsafe_allow_html=True)
                    else:
                        # Use the original generate_offer function for single model
                        st.info("Single model mode - multi-model comparison not available")
                        
                except Exception as e:
                    st.markdown(f"""
                    <div class="error-message">
                        ‚ùå Error generating offer: {str(e)}
                    </div>
                    """, unsafe_allow_html=True)
if st.button("üîç Debug RAG System"):
    debug_rag_system()

with col2:
    st.header("üìä Real-time Metrics")
    if getattr(st.session_state, 'metrics_history', []):
        latest_metrics = st.session_state.metrics_history[-1]
        
        # Check if multi-model evaluation was used
        if "multi_model_evaluation" in latest_metrics:
            # Multi-model metrics display
            st.subheader("üèÜ Best Model Results")
            best_model = latest_metrics.get("best_model", "unknown")
            model_data = latest_metrics.get("model_comparison", {}).get(best_model, {})
            
            st.metric("Best Model", best_model)
            st.metric("Quality Score", f"{model_data.get('quality_score', 0):.2f}/1.0")
            st.metric("Grade", model_data.get('grade', 'N/A'))
            st.metric("Estimated Cost", f"${model_data.get('cost', 0):.6f}")
            
            # Model comparison summary
            with st.expander("üìà All Models Summary"):
                for model, data in latest_metrics.get("model_comparison", {}).items():
                    col_model1, col_model2, col_model3 = st.columns(3)
                    with col_model1:
                        st.metric(f"{model} Quality", f"{data['quality_score']:.2f}")
                    with col_model2:
                        st.metric(f"{model} Grade", data['grade'])
                    with col_model3:
                        st.metric(f"{model} Cost", f"${data['cost']:.6f}")
        else:
            # Original single-model metrics display
            st.subheader("‚ö° Performance")
            st.metric("Latency", f"{latest_metrics['performance']['latency_seconds']}s")
            st.metric("Memory Used", f"{latest_metrics['performance']['memory_used_mb']} MB")
            
            st.subheader("üí∞ Cost")
            st.metric("Total Cost", f"${latest_metrics['cost']}")
            
            st.subheader("üéØ Quality")
            quality = latest_metrics['quality']
            st.metric("Overall Score", f"{quality['overall_score']}/1.0")
            st.metric("Grade", quality['grade'])
    else:
        st.info("Generate an offer to see metrics")


# Required imports (add these at the top of your file if not already present)
import streamlit as st
import plotly.express as px
import pandas as pd
import json
from datetime import datetime

# Initialize session state variables (add this before the analytics dashboard)
def initialize_session_state():
    if 'rag_system_initialized' not in st.session_state:
        st.session_state.rag_system_initialized = False

    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []

    if 'metrics_history' not in st.session_state:
        st.session_state.metrics_history = []

# Call initialization function
initialize_session_state()

# Analytics dashboard
st.header(" Analytics Dashboard")

# Use safe session state access
conversation_history = getattr(st.session_state, 'conversation_history', [])
metrics_history = getattr(st.session_state, 'metrics_history', [])

if conversation_history:
    # Tabs for different analytics
    tab1, tab2, tab3, tab4 = st.tabs([" Overview", " Cost Analysis", " Quality Trends", " History"])

    with tab1:
        col_met1, col_met2, col_met3 = st.columns(3)

        with col_met1:
            total_questions = len(conversation_history)
            st.metric("Total Questions", total_questions)

        with col_met2:
            if metrics_history:
                avg_cost = sum(m['cost'] for m in metrics_history) / len(metrics_history)
                st.metric("Average Cost", f"${avg_cost:.4f}")
            else:
                st.metric("Average Cost", "$0.0000")

        with col_met3:
            if metrics_history:
                avg_quality = sum(m['quality']['overall_score'] for m in metrics_history) / len(metrics_history)
                st.metric("Average Quality", f"{avg_quality:.2f}")
            else:
                st.metric("Average Quality", "0.00")

        # Latency chart
        if metrics_history:
            latencies = [m['performance']['latency_seconds'] for m in metrics_history]
            fig_latency = px.line(
                x=range(1, len(latencies) + 1),
                y=latencies,
                title="Response Latency Over Time",
                labels={'x': 'Question Number', 'y': 'Latency (seconds)'}
            )
            st.plotly_chart(fig_latency, use_container_width=True)
        else:
            st.info("No latency data available yet.")

    with tab2:
        if metrics_history:
            # Cost breakdown
            costs = [m['cost'] for m in metrics_history]
            fig_cost = px.bar(
                x=range(1, len(costs) + 1),
                y=costs,
                title="Cost per Question",
                labels={'x': 'Question Number', 'y': 'Cost ($)'}
            )
            st.plotly_chart(fig_cost, use_container_width=True)

            # Cumulative cost
            cumulative_cost = [sum(costs[:i+1]) for i in range(len(costs))]
            fig_cumulative = px.line(
                x=range(1, len(cumulative_cost) + 1),
                y=cumulative_cost,
                title="Cumulative Cost",
                labels={'x': 'Question Number', 'y': 'Cumulative Cost ($)'}
            )
            st.plotly_chart(fig_cumulative, use_container_width=True)
        else:
            st.info("No cost data available yet.")

    with tab3:
        if metrics_history:
            # Quality scores over time
            quality_scores = [m['quality']['overall_score'] for m in metrics_history]
            fig_quality = px.line(
                x=range(1, len(quality_scores) + 1),
                y=quality_scores,
                title="Quality Score Trends",
                labels={'x': 'Question Number', 'y': 'Quality Score'}
            )
            st.plotly_chart(fig_quality, use_container_width=True)

            # Quality distribution
            grades = [m['quality']['grade'] for m in metrics_history]
            grade_counts = pd.Series(grades).value_counts()
            fig_grades = px.pie(
                values=grade_counts.values,
                names=grade_counts.index,
                title="Quality Grade Distribution"
            )
            st.plotly_chart(fig_grades, use_container_width=True)
        else:
            st.info("No quality data available yet.")

    with tab4:
        # Conversation history
        st.subheader("Conversation History")
        for i, conv in enumerate(reversed(conversation_history)):
            with st.expander(f"Question {len(conversation_history) - i}: {conv['question'][:50]}..."):
                st.write(f"**Timestamp:** {conv['timestamp']}")
                st.write(f"**Question:** {conv['question']}")
                st.write(f"**Quality Score:** {conv['result']['metrics']['quality']['overall_score']}")
                st.write(f"**Cost:** ${conv['result']['metrics']['cost']}")
                st.write("**Generated Offer:**")
                st.text(conv['result']['generation'][:500] + "..." if len(conv['result']['generation']) > 500 else conv['result']['generation'])

        # Export functionality
        if st.button(" Export History"):
            export_data = {
                "conversation_history": conversation_history,
                "metrics_history": metrics_history,
                "export_timestamp": datetime.now().isoformat()
            }
            st.download_button(
                label="Download History (JSON)",
                data=json.dumps(export_data, indent=2),
                file_name=f"rag_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json"
            )

else:
    st.info("No data available yet. Generate some offers to see analytics!")

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; margin-top: 30px;">
    <p> RAG Cybersecurity AI Offer Generator | Built with Streamlit & Cohere</p>
    <p>For support, contact your system administrator</p>
</div>
""", unsafe_allow_html=True)

# Clear history button
if st.button(" Clear All History", type="secondary"):
    st.session_state.conversation_history = []
    st.session_state.metrics_history = []
    st.success("History cleared!")
    st.rerun()

